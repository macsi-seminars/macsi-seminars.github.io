date,time,month,day_of_week,week_no,presenter,affiliation,departmental_contact,title,abstract,google_scholar_profile,link_to_paper,office_required_y_n_and_if_yes_for_what_days,office_booked_y_n,do_they_require_accommodation_y_n,no_nights,from_when,to_when,accom_booked,gender_m_f_pnts,any_special_requirement_for_presentation,anything_else_we_should_know,masci_web_portal_updated,s_and_e_events_updated_events_se_ul_ie,date_fancy
2025-02-06,11am,Feb,Thursday,2,Natalia Kopteva,University of Limerick,NA,"Fractional differential equations: gentle introduction and gentle numerical analysis
","Over the past decade, there has been a growing interest in evolution equations of parabolic type that involve fractional-order derivatives in time of order in (0, 1). Such equations, also called subdiffusion equations, arise in various applications in engineering, physics, biology and finance. Hence, it is quite important to develop efficient and reliable computational tools for their numerical solution.

In this talk, I will touch on similarities and differences between fractional-parabolic equations and their classical counterparts, including the non-local nature of fractional-order derivatives, initial-time solution singularities, and slower long-term solution decay, as well as the proofs of some regularity properties and maximum principles. Then we shall consider some robust numerical methods for such equations, as well as the derivation of a-priori and a-posteriori estimates of the computational errors.

Compared to my recent talks on this topic (which some in the audience may have attended), I plan to give more focus to the numerical solution of such equations, as well as some basic ideas in their numerical analysis.",NA,NA,NA,NA,N,NA,NA,NA,NA,F,NA,NA,NA,NA,Thu 06 Feb 2025
2025-02-20,11am,Feb,Thursday,4,Davood Roshan,University of Galway,Shirin Moghaddam,"Challenge Your Limits!","Interval estimation is a cornerstone of statistical inference. Regardless of whether an interval has been derived using a frequentist, Bayesian or computational approach the ‘practical’ interpretation is the same, a set of values ’likely’ to contain the parameter of interest. 
Interval estimation in introductory statistics courses often involves frequentist approaches for one/two sample problems. A common misuse, particularly when comparing two treatments, is when a Confidence Interval is used instead of a Prediction Interval in estimating the likely improvement for a future individual receiving the treatment of interest. Prediction Intervals however are typically introduced later in the curriculum as a component of simple linear regression. 
Other solution to consider is to report Tolerance Intervals which aims to provide a range of values for the treatment effect for a certain percent of the population of interest. Despite their appeal, Tolerance Intervals are rarely included in introductory courses. They may feature in some medical statistics courses as population-based reference intervals to interpret a set of laboratory test results for a particular individual. 
In this presentation, a review and discussion of the use and misuse of confidence, prediction, and tolerance intervals and their place in the curriculum will be discussed. New approaches for generating personalised adaptive reference intervals for longitudinal monitoring will be also presented. Such adaptive reference intervals will adapt successively whenever a new measurement is recorded for an individual by accounting for both the between and within individual variability.",NA,NA,Y,NA,Y,1,NA,NA,Y,M,NA,NA,NA,NA,Thu 20 Feb 2025
2025-03-06,11am,Mar,Thursday,6,Mainendra Dewangan,University of Limerick,Mehakpreet Singh,Effects of Topographical Wall Patterns on Flow through Porous Media,"The Darcy-Brinkmann (DB) model advances the understanding of fluid flow through a porous medium by extending the foundational principles of Darcy's law to include viscous shear effects. This enhancement is crucial for accurately analyzing flows that are significantly influenced by both the porous characteristics and the viscous shear (Brinkmann effect). Integration of these effects makes the DB model a more comprehensive tool for studying complex flow dynamics. Exploring fluid flow in porous media is essential for designing and optimizing various engineering systems, including filtration devices, chemical reactors, and technologies for cooling microelectronics. With this motivation, the present study investigates the flow characteristics of a viscous, incompressible, steady, and Newtonian fluid through an undulating microchannel containing a porous medium. The flow is governed by the Darcy–Brinkman model with no-slip boundary conditions at walls. The primary objective of this study is to develop theoretical and computational models for flow parameters that are independent of the permeability of porous medium and to extend the scope of previous studies. Lubrication theory is employed to analyze key flow parameters such as flow rate, velocity, and wall shear stress in complex-shaped microchannels. However, the spectral method is applied to a sinusoidal microchannel to overcome the limitations of lubrication and boundary perturbation methods. The present study reveals that flow parameters are significantly influenced by dimensionless quantities such as pattern amplitude, wavelength, and permeability (κ). The spectral model predicts nonlinear flow rate behaviour at high permeability (κ ≫ 1 ) and accurately captures flow rate transitions in the Darcian flow regime across various wavelengths, unlike other theoretical approaches. Conversely, for low permeability (κ ≪ 1) in the Stokes flow limit, the flow rate behaviour remains monotonic for both small and large wavelengths. The spectral model exhibits good reliability compared to classical lubrication theory, extended lubrication theory, and boundary perturbation methods, particularly for large values of dependent variables. Predictions from the spectral approach show good agreement with numerical results across a broad range of parameters. A detailed analysis of the influence of various parameters on flow quantities is examined. The findings of the present study are highly significant and have broad applications in multiple fields, such as chemical reactors, filtering equipment, heat exchangers, and flow through rock fractures.",https://scholar.google.co.in/citations?hl=en&user=jUEUMhYAAAAJ&view_op=list_works&sortby=pubdate,NA,N,NA,N,NA,NA,NA,NA,M,NA,NA,NA,NA,Thu 06 Mar 2025
2025-03-19,2pm,Mar,Wednesday,8,Iain Moyles,York University,Andrew Fowler,The Only Thing to Fear is Fear Itself,"We present a disease model with behavioural impact in the form of fear of infection. This fear induces prophylactic action in the form of contact reduction. Increase in fear is driven by rising case numbers while decrease in fear occurs both naturally over time and with increasing recovery of infection. We show that our model exhibits two limits, one where fear can be effective at mitigating the disease and one where it is not. When fear is an effective tool, we analyze its limitations in stopping disease spread.",NA,NA,Y,Y,N,NA,NA,NA,NA,M,NA,NA,NA,NA,Wed 19 Mar 2025
2025-04-03,11am,Apr,Thursday,10,Robert Garvey,University of Limerick,Michael Vynnycky,Hydrodynamics and drug dissolution in USP4,"The dissolution of solid spherical particles is a canonical problem found in many applications. Of particular interest is the dissolution of drug particles in the pharmaceutical industry. In-vitro dissolution testing is crucial in investigating drug quality, stability, and regulatory compliance. The United States Pharmacopoeia set guidelines for testing dissolution and outlines seven main testing apparatus. One such widely utilised apparatus is the flow through device, USP4. The USP4 apparatus comprises of three main components: a reservoir containing the dissolution medium; a pump responsible for controlling the flow rate of the fluid; and a flow-through cell where the drug sample is situated as dissolution occurs.

The solvent fluid is ordinarily pumped in a semi-sinusoidal manner or at a constant flow rate. This flow rate is impactful on dissolution characteristics. Mathematical modelling may be utilised to understand the complex interaction between the hydrodynamics and the dissolution process within the USP4 system. We investigate existing models; such models are solved numerically. We use non-dimensionalisation and asymptotic methods to derive analytical asymptotic solutions to these existing model equations. Our analytics are compared with numerical results and found to give good agreement; such analytical solutions are particularly valuable due to the time-consuming nature of solving these equations numerically, owing directly to the presence of the Basset history integral force. This Basset history integral may be interpreted as a fractional derivative.

Existing models are unable to accurately capture all experimental data. We discuss recent developments on the modelling of hydrodynamics and dissolution and outline potential improvements.",NA,NA,N,NA,N,NA,NA,NA,NA,M,NA,NA,NA,NA,Thu 03 Apr 2025
2025-04-04,11am,Apr,Friday,10,Colm Mulcahy,Spelman College,Romina Guborro,"One, Two, Many (or a dozen reasons why mathematics isn’t as easy as 1,2,3)","Are there really primitive tribes whose system of counting goes: ""One, Two, Many"", indicating that

from three on it’s more or less a blur? Maybe we modern humans are such a tribe. Despite the

sophistication we see in ourselves compared with our less advanced ancestors from times long past,

it’s surprising how little progress we’ve made in addressing some basic problems in 3D or beyond, or

when solving seemingly simple equations in 3 or more variables. For despite our astonishing mastery

of some ABCs, such as Air (flight, weather prediction), Biology (medical breakthroughs, DNA) and

Communications (phone, video, email), we often struggle to get past 1, 2, 3 in other domains. Or

sometimes even to get to 3. We'll survey a dozen fun topics in shapes and numbers and patterns

whose basics and generalization can be explored with little mathematical background, and which

speedily lead to ""what if"" questions ranging from easy to tricky to ""we just don't know."" Coins, cakes,

fruit, bagels, cubes, squares and primes will all make an appearance. The late Martin Gardner, whose writings  turned  so many  on to maths, knew all too well that such playful queries

can both excite students about mathematics and lead to real research at the frontiers of the subject.

There will be satisfying Aha! moments, there will be room for innovative ideas, and million dollar

prizes will be discussed.",NA,NA,Y,NA,Y,1,9th of April,10th of april,NA,F,NA,NA,NA,NA,Fri 04 Apr 2025
2025-04-10,11am,Apr,Friday,11,Colm Connaughton,London Mathematical Laboratory,Padraig MacCarron,"Rationality, decision theory and ergodicity","In decision theory, choice in the presence of uncertainty - think of deciding whether or not to place a bet on a horse - is typically modelled as a process of expected utility maximisation. Agents are assumed to possess an intrinsic  utility function that ascribes a ""value"" to each possible outcome. An uncertain choice is then considered rational if it maximises the expected utility of the set of possible outcomes. In this talk, I will describe some examples of repeated gambles which illustrate some inconsistencies with this notion of rationality. For additive, gambles, where the potential payoffs are independent of an agent's current wealth, all seems to work fine. In contrast, for multiplicative gambles, where payoffs are proportional to an agent's current wealth, one encounters situations in which the expected utility grows exponentially with the number of rounds played while the typical wealth goes to zero with probability one. The mathematical root of the problem lies in the interchangeability (or not) of the ensemble  average outcome and the time average outcome - what physicists refer to as ergodicity. The evolution of an agent's wealth during a sequence of additive gambles can be modelled as a simple random walk which has ergodic increments. A sequence of multiplicative gambles, however, should be modelled as a geometric random walk whose increments strongly break ergodicity. The failure to properly account for this distinction can explain many so-called irrationalities in decision theory and behavioural economics. If time allows, I will also describe some more technical work constructing a stochastic process that interpolates smoothly between the additive and multiplicative random walks. The aim is to better understand the nature of the ergodicity breaking observed in the multiplicative case.",NA,NA,Y,NA,Y,1,10th of April,11th of April,Y,F,NA,NA,NA,NA,Thu 10 Apr 2025
2025-04-11,12pm,Apr,Friday,11,Sarah Heaps,Durham University,James Sweeney,"Bayesian inference of sparsity in stationary, multivariate autoregressive processes","In many fields, advances in sensing technology have made it possible to collect large volumes of time-series data on many variables. In a diverse array of fields such as finance, genetics and neuroscience, a key question is whether such data can be used to learn directed relationships between variables. In other words, do changes in one variable consistently precede those in another? Graphical vector autoregressions are a popular tool for characterising directed relationships in multivariate systems because zeros in the autoregressive coefficient matrices have a natural graphical interpretation in terms of the implied Granger (non)-causality structure. In many applications, it is natural to assume that the underlying process is stable so that, for example, uncertainty in forecasts does not increase without bound as the forecast horizon increases. Though stationarity is commonly stated as an assumption, it is generally not enforced as a constraint because enforcing stability demands restricting the autoregressive coefficient matrices to lie in a constrained space, with a complex geometry, called the stationary region. This is problematic because the number parameters grow quadratically with dimension, making it increasingly difficult to learn, with certainty, that a process is stationary. Working in the Bayesian paradigm, we use a parameter expansion approach to tackle the problem of inference for sparse and stable vector autoregressions by constructing a spike-and-slab prior with support constrained to the stationary region. Computational inference is carried out via a Metropolis-within-Gibbs scheme which uses Hamiltonian Monte Carlo to draw from the full conditional distribution of the continuous parameters. To illustrate our approach, we consider long-term spatio-temporal data on interictal epileptic activity (IEA), which are abnormal brain activity patterns mostly seen in people with epilepsy. Learning about the drivers of variability in this application has the potential to transform epilepsy treatment as the IEA rate is thought to underpin the cognitive deficits in this cohort.",NA,NA,Y,NA,Y,NA,NA,NA,NA,M,NA,NA,NA,NA,Fri 11 Apr 2025
2025-04-17,11am,Apr,Thursday,12,Edward Gunning,University of Pennsylvania,Norma Bargary,Non-linear Latent Representations in Functional Data Modeling,"Latent feature representations (e.g., PCA) are widely used for dimensionality reduction and statistical modeling of high-dimensional functional and multivariate data. Traditional functional regression models typically rely on linear basis expansions (e.g., PCA, B-splines, wavelets), but modern non-linear machine learning methods (e.g., autoencoders, GANs) offer more flexible alternatives.
In this work, we present two main contributions:
1.  We propose CLaRe (Compact near-Lossless Latent Representations), a flexible evaluation framework for selecting among linear and non-linear latent feature representations in high-dimensional functional and multivariate data. CLaRe provides a principled set of criteria to assess methods based on their dimensionality reduction (compactness) and the information they preserve (near-losslessness).
2. We demonstrate how, when non-linear methods such as autoencoders are selected, they can be embedded within the Functional Mixed Model (FMM) framework of Morris and Carroll (2006). This integration enables flexible modeling of complex functional structures while retaining the interpretability and inference capabilities of FMMs. We illustrate the utility of this approach on multidimensional functional imaging data.",NA,NA,Y,Y,Y,2,NA,NA,Y,M,NA,NA,NA,NA,Thu 17 Apr 2025
2025-04-24,11am,Apr,Thursday,13,Leonard Henckel,University College Dublin,Kevin Burke,Graphical tools for selecting conditional instrumental sets,"Causal inference studies how to use statistical tools to infer the properties of an unobserved distribution corresponding to a hypothetical experiment from observational data. A popular tool for causal inference are instrumental variables; auxiliary variables that affect the treatment in a way that mimics an experiment. However, it is also well-known that instrumental variable estimators tend to be inaccurate and that their accuracy heavily depends on the choice of instrument. We consider the problem of how to characterize which auxiliary variables result in efficient estimators under the assumption that we are given approximate knowledge of the underlying causal structure in the form of a causal graph. We first characterize the class of all valid conditional instrumental sets for the target causal effect and provide a graphical criterion that, for certain pairs, identifies which of the two corresponding estimators has the smaller asymptotic variance. We then use these two results to characterize a valid conditional instrumental set for which the corresponding estimator has the smallest asymptotic variance that can be ensured with a graphical criterion.  
",NA,NA,Y,Y,Y,2,NA,NA,Y,F,NA,NA,NA,NA,Thu 24 Apr 2025
2025-05-08,11am,May,Thursday,15,Miriam Casey,University College Dublin,David O'Sullivan,Application of mathematical modelling to infectious animal disease challenges,"Inferring hidden infectious disease processes through integration of quantitative, biological, and socioeconomic expertise can inform policy for effective disease control. I will present snapshots of my previous interdisciplinary research into foot-and-mouth disease and Covid-19 and its subsequent impacts. The focus of the presentation will be my current work applying a similar approach to bovine tuberculosis in Ireland. 
In Ireland, Mycobacterium bovis is the most prevalent of the zoonotic Mycobacterium tuberculosis complex bacteria in animals, and causes bovine tuberculosis (bTB) in cattle. The bTB control programme cost approximately €2 billion between 2013 and 2023, and increasing levels of bTB threaten our cattle product export market access which, in 2022, was worth approximately €9.45 billion. Uncertainties relating to complex multi-species epidemiology of bTB, highly imperfect diagnostic testing, in association with robust stakeholder debate about cattle and wildlife related control measures, create disease control challenges. Through fitting a stochastic mechanistic model to bTB breakdown data in 9,137 Irish cattle herds, we aimed to improve our understanding of M. bovis transmission within herds, to estimate where in the diagnostic testing process infected cattle are being missed, and to enable exploration of the potential impact of interventions. The current best-fit model uses the susceptible-occult-reactive compartmental framework, animal level heterogeneity in transmission, and allows for reduced test sensitivity prior to initial case diagnosis in each herd-level skin test event. This framework, using parameters inferred by Approximate Bayesian Computation (ABC), produces simulations with an acceptable fit to 23 target metrics in the data.   I will present our results, including new evidence about mechanisms relating to M. bovis transmission and diagnosis. I will also describe the computational and statistical challenges and opportunities highlighted by the project and future research plans.
",NA,NA,NA,NA,Y,NA,NA,NA,NA,M,NA,NA,NA,NA,Thu 08 May 2025
2025-05-22,2pm,May,Thursday,17,Jacob Curran-Sebastian,University of Copenhagen,James Gleeson,Modeling the importation and transmission dynamics of a novel pathogenic strain using branching processes.,"The importation and subsequent establishment of novel pathogenic strains in a population is subject to a large degree of uncertainty due to the stochastic nature of the disease dynamics. Mathematical models need to take this stochasticity in the early phase of an outbreak in order to adequately capture the uncertainty in disease forecasts. We start with a simple branching process model of disease spread, which we then build upon to include within-host level heterogeneity and a time-varying transmission rate. This flexible approach can be straightforwardly tailored to capture the salient aspects of a disease outbreak. We combine this with a model of case importation that occurs via an independent marked Poisson process.

We use this framework to investigate the impact of different control strategies, particularly on the time to establishment of an invading, exogenous strain. We also demonstrate the relationship between our model and deterministic approximations, such that longer term projections can be generated that still incorporate the uncertainty from the early growth phase of the epidemic. Our approach produces meaningful projections for a disease outbreak that are applicable in a wide variety of settings. These can be used by policymakers, for example, to estimate the length time for which interventions need to be in place in order to achieve either local elimination of a disease or a delay in the time at which the disease becomes established in the population.",NA,NA,Y,NA,Y,2,NA,NA,NA,M,NA,NA,NA,NA,Thu 22 May 2025
2025-05-28,12pm,May,Wednesday,18,Jean-Gabriel Young,University of Vermont,David O'Sullivan,Designing interventions with message passing on clustered graphs,"Models of contagions can describe many network dynamics, such as cascading failures in economic systems, information diffusion, and pathogen transmission. They figure prominently in intervention design problems—for example, deciding how to roll out vaccines or designing robust economic systems. The idea is, roughly, to test counterfactuals provided by realistic contagion models and tune the system to increase the likelihood of favorable outcomes. Unfortunately, this process involves costly numerical optimization and calculations. In this talk, I will first review the message passing framework, which is routinely used to tame the intervention design problem's complexity. I will then highlight some well-known issues with this framework, chiefly that it overestimates the likelihood of a contagion spreading. I will then discuss two promising approaches for correcting this issue: the neighborhood message passing (NMP) framework and a new graph machine learning algorithm.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,Wed 28 May 2025
